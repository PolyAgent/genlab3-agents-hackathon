{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_zGwdPymdLi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install qdrant-client pymongo\n",
        "!pip install langchain_community pypdf langchain_text_splitters langchain_experimental\n",
        "!pip install llama-index llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed qdrant_client fastembed\n",
        "!pip install openai==1.14.1 langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATagQ1NdsHFw"
      },
      "source": [
        "# Qdrant Client (Retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "216199f1a1294fff872b1c28cb54c627",
            "5c10344b10f74299ae6ebaffeedd1738",
            "dd90efc9f84f40bd84c01fd58b71be0c",
            "dd53297019734702a192656b88fd8950",
            "72e069e47d524f25aaf4ee0f0cc03eb7",
            "1887361a8bcb4ce2b4a067ed965a9dd3",
            "b7c9b3939d364424a250979c40e0e4b2",
            "de7acd9af01a42bd8f5e539d959484c2",
            "9c86150808174e1090089321c4f41a6c",
            "70ba4e735abe4527a79f13d708603f21",
            "19eb0fc431fa442498e5e025d28e54cb"
          ]
        },
        "id": "UOdLD99EoL3p",
        "outputId": "af8212ed-4e15-4142-dc1b-f912bd5bd405"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "216199f1a1294fff872b1c28cb54c627"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import qdrant_client\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Document, Settings\n",
        "import os\n",
        "from qdrant_client import QdrantClient\n",
        "import openai\n",
        "\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure\n",
        "\n",
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "\n",
        "\n",
        "COLLECTION_NAME = \"vc-pilot-full\"\n",
        "\n",
        "def get_indexer_and_retriever():\n",
        "  # Set up Qdrant client for vector store\n",
        "    qdrant_client = QdrantClient(\n",
        "        url='https://56ab7b97-f618-4723-9b13-93e0b140c31b.us-east4-0.gcp.cloud.qdrant.io:6333',\n",
        "        api_key=\"lBfwfnQqD1ogXoZ86-X8wFWigRoZwZk1ydOYJf4pv6WJyLQmm7jqDg\",\n",
        "    )\n",
        "\n",
        "    # Embedding model for vector insertion\n",
        "    os.environ[\"OPENAI_API_BASE\"]=\"https://api.fireworks.ai/inference/v1\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] =\"soXg5G0dRAywvZyyLIOLwjKBfe6S1kqV6lC2KdEcVbWqlxM4\"\n",
        "    embed_model = FastEmbedEmbedding(model_name=\"nomic-ai/nomic-embed-text-v1.5\")\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    vector_store = QdrantVectorStore(\n",
        "        client=qdrant_client,\n",
        "        collection_name=COLLECTION_NAME\n",
        "    )\n",
        "\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "    retriever = index.as_retriever()\n",
        "    return index, retriever\n",
        "\n",
        "index, retriever = get_indexer_and_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dvcw6L74wEsB"
      },
      "outputs": [],
      "source": [
        "# https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0ZW43LsAgb"
      },
      "source": [
        "## Mongo Reader (Ingestor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dwKiXJjDsC1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9325edbf-657f-4ce6-ae48-ee6e15264e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc count 22996\n"
          ]
        }
      ],
      "source": [
        "class MongoDBQuery:\n",
        "    def __init__(self, db_name, collection_name, uri=\"mongodb://localhost:27017/\"):\n",
        "        self.uri = uri\n",
        "        self.db_name = db_name\n",
        "        self.collection_name = collection_name\n",
        "        self.client = None\n",
        "        self.db = None\n",
        "        self.collection = None\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.client = MongoClient(self.uri)\n",
        "            self.client.admin.command('ping')\n",
        "            self.db = self.client[self.db_name]\n",
        "            self.collection = self.db[self.collection_name]\n",
        "        except ConnectionFailure:\n",
        "            print(\"Failed to connect to MongoDB\")\n",
        "            raise\n",
        "\n",
        "    def query(self, query_filter=None):\n",
        "        if query_filter is None:\n",
        "            query_filter = {}\n",
        "        try:\n",
        "            results = self.collection.find(query_filter)\n",
        "            return list(results)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the query: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "class FireworkLLM(LLM):\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Firework\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        client = openai.OpenAI(\n",
        "            base_url = \"https://api.fireworks.ai/inference/v1\",\n",
        "            api_key=\"soXg5G0dRAywvZyyLIOLwjKBfe6S1kqV6lC2KdEcVbWqlxM4\",\n",
        "        )\n",
        "        response = client.chat.completions.create(\n",
        "          model=\"accounts/fireworks/models/mixtral-8x7b-instruct\",\n",
        "          temperature=0,\n",
        "          max_tokens=16000,\n",
        "          messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "          }],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def get_mongo_content():\n",
        "    mongo = MongoDBQuery(\n",
        "        db_name=\"arxiv\",\n",
        "        collection_name=\"papers_for_review\",\n",
        "        uri=\"mongodb+srv://genlab-hackathon:qSzbc3NWGgWie1aP@age-house.dypq7r5.mongodb.net\",\n",
        "    )\n",
        "    mongo_content = mongo.query(query_filter={\"abstract\": {\"$exists\": True}})\n",
        "    print(\"Doc count\", len(mongo_content))\n",
        "    return mongo_content\n",
        "\n",
        "\n",
        "llm = FireworkLLM()\n",
        "mongo_content = get_mongo_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky3Y0B1U6tpB"
      },
      "source": [
        "## Insert Mongo Documents into indexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "c7cb02b343bb45b4a095d26caec0c2fb",
            "5b47dbe2e63e4151b1b6c40e33f53ddd",
            "69c44660f89a4598b3fe8cdd515d3c52",
            "c9ca7f6af1794655ba3879da496a6f77",
            "833aa6ac259d4b7a879327399fd95c0e",
            "bec85f1bc6644323bd99ca672c79b941",
            "a14f476418224be29dca0c795e61e30f",
            "766f136cf3b44efda50db7511e7a548b",
            "6d168d1f438448359da5a329d100507d",
            "1e3fe09e691c4faa9582082ddfd6ede0",
            "f6fe02a1134a49409cda85223f21935f"
          ]
        },
        "id": "IH4dIcn04qO_",
        "outputId": "b2cc58b7-4392-4493-dfc4-cb5de8569d22"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/22996 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7cb02b343bb45b4a095d26caec0c2fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8dbe370b51ff>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Wait for all futures to complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8dbe370b51ff>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmax_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Number of available CPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmongo_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0muse_abstracts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0muse_papers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "from threading import Lock\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "def process_paper(lock, progress_bar, paper, use_abstracts=True, use_papers=False):\n",
        "    if use_abstracts:\n",
        "        title = paper[\"title\"]\n",
        "        text_content = paper[\"abstract\"]\n",
        "        doc = Document(text=f\"{title}:\\n{text_content}\")\n",
        "        index.insert(doc)\n",
        "    if use_papers:\n",
        "        pdf_loader = PyPDFLoader(paper[\"pdf_url\"])\n",
        "        documents = pdf_loader.load_and_split(text_splitter=text_splitter)\n",
        "        index.insert(documents)\n",
        "    with lock:\n",
        "        progress_bar.update(1)\n",
        "\n",
        "lock = Lock()\n",
        "\n",
        "max_workers = os.cpu_count()\n",
        "progress_bar = tqdm(total=len(mongo_content))\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    use_abstracts = True\n",
        "    use_papers = False\n",
        "    futures = [executor.submit(process_paper, lock, progress_bar, paper, use_abstracts, use_papers) for paper in mongo_content]\n",
        "\n",
        "    for future in as_completed(futures):\n",
        "        pass\n",
        "\n",
        "progress_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUjN16R6_wi-",
        "outputId": "d1f8d319-fe7c-4757-d073-909256baba19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LLM Multi-Agent Systems: Challenges and Open Problems: This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.',\n",
              " \"LLM-SAP: Large Language Model Situational Awareness Based Planning: This work pioneers evaluating emergent planning capabilities based on situational awareness in large language models. We contribute (i) novel benchmarks and metrics for standardized assessment; (ii) a unique dataset to spur progress; and (iii) demonstrations that prompting and multi-agent schemes significantly enhance planning performance in context-sensitive planning tasks. Positioning this within a situated agent and automated planning research, we highlight inherent reliability challenges--efficiently mapping world states to actions without environmental guidance remains open despite simulated domain advances. Although out-of-scope, limitations around validation methodology and data availability indicate exciting directions, including fine-tuning on expanded planning corpora and optimizations for triggering fast latent planning. By conclusively demonstrating current methods' promise and limitations via rigorous comparison, we catalyze investigating reliable goal-directed reasoning for situated agents.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def get_relevant_documents(question):\n",
        "    relevant_documents = list(map(\n",
        "            lambda node: node.text.replace(\"\\n\", \" \"),\n",
        "            sorted(\n",
        "              retriever.retrieve(question),\n",
        "              key=lambda node: node.score,\n",
        "              reverse=True\n",
        "            ),\n",
        "    ))\n",
        "    return relevant_documents\n",
        "\n",
        "# relevant_documents = get_relevant_documents(\"llm\")\n",
        "# relevant_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Lb5WBEVn2D9U"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfksqaJOFa_m",
        "outputId": "c1b3a9b5-3129-441f-d095-7c1e8857cbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPOSAL\n",
            "Personalized learning platform using AI to adapt to student's learning pace.\n",
            "TASKS\n",
            "[\"Effectiveness of AI-driven personalized learning platforms in improving student outcomes: Examining the impact of AI-based adaptive learning platforms on student performance and engagement is crucial to predict the startup's success.\", \"Comparison of AI-driven personalized learning platforms with traditional methods: Analyzing the differences in learning outcomes, user experience, and cost-effectiveness between the proposed AI solution and traditional learning methods will help assess the startup's value proposition.\", \"Data privacy and security in AI-powered personalized learning platforms: Investigating potential data privacy and security concerns, as well as regulatory compliance, is essential to ensure the startup's solution is viable and ethically sound.\", \"User experience and interface design in AI-driven personalized learning platforms: Understanding user preferences, usability, and accessibility aspects of the proposed platform will help predict the startup's success in attracting and retaining users.\", \"Scalability and adaptability of AI-powered personalized learning platforms in various educational settings: Evaluating the applicability and adaptability of the startup's solution in diverse educational contexts, such as K-12, higher education, and corporate training, will help predict the startup's growth potential and market reach.\"]\n",
            "CHUNKS\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from typing import List\n",
        "\n",
        "def get_research_tasks(proposal: str) -> List:\n",
        "    research_template = PromptTemplate.from_template(\n",
        "        template = \"\"\"\n",
        "    You are a research analyst for a venture capital investor that is evaluating AI technology startups.\n",
        "    You have expertise in AI field and you understand how to evaluate the risk profile for new startup ideas.\n",
        "    A startup is proposing the following idea:\n",
        "    {proposal}\n",
        "\n",
        "    Your task is to use the TRIZ methodology of problem solving to generate a list of the top 5 topics that would need to be researched.\n",
        "    The list should be prioritized based on factors that can predict the success of the startup and should be simple, concise.\n",
        "    This list will be used to search a database of 100,000 scientific papers on AI\n",
        "\n",
        "    Here is an example of independent reserach topics that may be pursued where the topic is a statup's proposal for funding on a 1-bit quantization model:\n",
        "    - Impact of 1-bit quantization on LLM performance and accuracy\": Understanding the trade-off between computational efficiency and model accuracy is crucial to predict the success of the startup's proposal.\n",
        "    - Scalability of 1-bit quantized LLMs in real-world applications\": The startup's success depends on the applicability and scalability of their solution in various industries and use-cases.\n",
        "    - Comparative analysis of 1-bit quantization with other model compression techniques\": It's essential to evaluate how the proposed solution compares to alternative methods in terms of efficiency, accuracy, and ease of implementation.\n",
        "    - Hardware and software requirements for implementing 1-bit quantized LLMs\": Understanding the infrastructure needed to support the startup's solution will help assess the overall cost, accessibility, and feasibility of their proposal.\n",
        "    - Case studies and success stories of 1-bit quantization in AI models\": Learning from previous experiences and successes can provide valuable insights into potential challenges, opportunities, and the overall viability of the startup's idea.\n",
        "\n",
        "    Respond in only `-` delimited list format with in depth independently operable research tasks.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    prompt = research_template.format(\n",
        "        proposal=proposal,\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    tasks = list(\n",
        "              filter(\n",
        "                  lambda y: y != \"\",\n",
        "                  map(\n",
        "                      lambda x: x.strip()\\\n",
        "                                 .replace(\"- \", \"\")\\\n",
        "                                 .replace(\"\\\"\", \"\"),\n",
        "                      response.split(\"\\n\")\n",
        "                      )\n",
        "                  )\n",
        "            )\n",
        "    return tasks\n",
        "\n",
        "\n",
        "# proposal = \"LLMs demand substantial computational power for both training and inference, presenting barriers to their widespread application. The proposition of 1-bit quantization seeks to address these challenges by compressing model size and enhancing computational efficiency.\"\n",
        "proposal = \"Personalized learning platform using AI to adapt to student's learning pace.\"\n",
        "tasks = get_research_tasks(proposal)\n",
        "chunks = get_relevant_documents(proposal)\n",
        "print(\"PROPOSAL\")\n",
        "print(proposal)\n",
        "print(\"TASKS\")\n",
        "print(tasks)\n",
        "print(\"CHUNKS\")\n",
        "print(len(chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuQrmURXjeyf"
      },
      "source": [
        "# Agent Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XTWZ0Rx3jftB"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"document_retriever\",\n",
        "    \"Search for information of latest computer science and language model research papers. For any questions about the latest research, use this tool.\",\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    retriever_tool,\n",
        "]\n",
        "\n",
        "\n",
        "agent_prompt = PromptTemplate(\n",
        "        input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
        "        template=\"\"\"\n",
        "Answer the following questions as best you can.\n",
        "You have access to the following tools that can reference the latest research on the given topic.\n",
        "Do not hallucinate.  If you do not have the relevant research to the question, exclusively say `NOT ENOUGH INFORMATION`:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        ")\n",
        "\n",
        "# react_prompt = hub.pull(\"hwchase17/react\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ob4JBkUOlU8Q"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "def get_agent_executor(llm, tools, agent_prompt):\n",
        "\n",
        "    agent = create_react_agent(\n",
        "      llm, tools, agent_prompt\n",
        "    )\n",
        "    agent_executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        verbose=False,\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "\n",
        "    return agent_executor\n",
        "\n",
        "\n",
        "proposal = \"Our AI startup is revolutionizing information retrieval with our state-of-the-art cross-encoder for retrieval-augmented generation. By integrating advanced natural language processing techniques, our technology understands and processes complex queries to deliver precise, contextually relevant information in real-time. Designed for scalability, our solution enhances search engines, recommendation systems, and data analysis tools, providing unmatched accuracy and speed in retrieving the exact information or content users need. Our mission is to empower businesses and developers to unlock the full potential of their data, making information access seamless and more intuitive than ever before.\"\n",
        "# proposal = \"Improving accuracy of LLM using RAG\"\n",
        "\n",
        "agent_executor = get_agent_executor(llm, tools, agent_prompt)\n",
        "tasks = get_research_tasks(proposal)\n",
        "# print(f\"PROPOSAL: {proposal}\")\n",
        "# print(\"~\"*20)\n",
        "\n",
        "def get_research(proposal, agent_executor, tasks):\n",
        "    summaries = []\n",
        "    citations = retriever.retrieve(proposal)\n",
        "    for i, task in enumerate(tasks):\n",
        "        response = agent_executor.invoke({\"input\": task})\n",
        "        summary = response[\"output\"]\n",
        "        summaries.append(summary)\n",
        "        # print(f\"TASK {i+1}: {task}\")\n",
        "        # print(f\"RESEARCH: {summary}\")\n",
        "        # # print(f\"CONTENT {citations}\")\n",
        "        # print(\"#\"*20)\n",
        "    return summaries, citations\n",
        "\n",
        "# summaries, citations = get_research(proposal, agent_executor, tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_3KZZ_UcskmB"
      },
      "outputs": [],
      "source": [
        "def generate_highlights(proposal, citations, summaries):\n",
        "    highlights_prefix = f\"\"\"\n",
        "You are a research analyst working for a venture capital firm and you need to assess a risk profile of a deep tech startup that is working on the following proposal:\n",
        "{proposal}\n",
        "\"\"\"\n",
        "\n",
        "    highlights_chunks = \"\"\n",
        "    for i, (chunk, summary)  in enumerate(zip(chunks, summaries)):\n",
        "        highlights_chunks += f\"\"\"\n",
        "Citation {i+1}:\n",
        "{chunk}\n",
        "\n",
        "Summary {i+1}:\n",
        "{summary}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    highlights_suffix = \"\"\"\n",
        "Using these citations and summaries only - please provide a final risk report analysis using this template:\n",
        "\n",
        "### Identify Key Challenges\n",
        "### Explore Potential Solutions\n",
        "### Assess Innovation and Trend Alignment\n",
        "### Solution Potential and Risk Evaluation\n",
        "\n",
        "ONLY OUTPUT CONTENT WITHIN THESE 4 SECTIONS\n",
        "Respond in only `-` delimited list format with 3-5 items in EACH SECTION.\n",
        "\"\"\"\n",
        "    highlights_prompt = \"\\n\".join([highlights_prefix, highlights_chunks, highlights_suffix])\n",
        "    response = llm.invoke(highlights_prompt)\n",
        "    return response\n",
        "\n",
        "# highlights_response = generate_highlights(proposal, citations, summaries)\n",
        "# print(highlights_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eExUoQvdsn2I"
      },
      "outputs": [],
      "source": [
        "def get_followup_questions(highlights_response):\n",
        "    followup_questions_prompt = f\"\"\"\n",
        "    You are a risk analyst working in a venture capital evaluating a tech startup. You have done the research on the technology and generated a risk report. Based on the risk report below, come up with 10 follow-up questions directed towards the startup founders. Be concise and critical or a kid would die. The questions cannot be answered by evaluating the risk report and should only address the important points. Limit the question to a sentence and less than 100 words.\n",
        "    {highlights_response}\n",
        "    \"\"\"\n",
        "    followup_response = llm.invoke(followup_questions_prompt)\n",
        "    return followup_response\n",
        "\n",
        "# followup_response = get_followup_questions(highlights_response)\n",
        "# print(followup_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_problem_statement(proposal):\n",
        "    problem_statement_prompt = f\"\"\"\n",
        "    Given a press release from a startup, create a statement describing problem and proposed solution that the startup is trying to address in detail. Be concise and targeted or kids would die. Only return the statement, and keep the statement one sentence. Summarize the information and do not include specific numbers or data.\n",
        "    {proposal}\n",
        "    \"\"\"\n",
        "    problem_statement = llm.invoke(problem_statement_prompt)\n",
        "    # print(problem_statement)\n",
        "    return problem_statement\n",
        "\n",
        "def get_conclusion(proposal, highlights_response):\n",
        "    conclusion_prompt = f\"\"\"\n",
        "    You are a research analyst working on a due diligence report for a venture capital firm and need to assess a technology risk profile of a deep tech AI startup that is working on\n",
        "    {proposal}\n",
        "\n",
        "    You've read all the research papers relevant to this topic, produced a preliminary report, formulated relevant questions and want to create a due diligence conclusion summary based on that. Please write a conclusion section for the report, focus on investment risks of our firm, a venture capital firm investing into early stage startups, keep it under 200 words and focus on technology risk. Be really critical analyst, like someone's job depends on this. Be dry and to the point, less verbose and more factual. And then include one-liner to describe the risk profile like you are describing it to a friend, super simple and with a bit of a humor.\n",
        "    {highlights_response}\n",
        "\n",
        "    Respond in the following format:\n",
        "    Conclusion: <serious conclusion summary>\n",
        "    Final Thoughts: <goofy analogy to a friend>\n",
        "    Rating: <1.0 - 10.0>\n",
        "    Emoji: <relevant emoji>\n",
        "    \"\"\"\n",
        "\n",
        "    conclusion = llm.invoke(conclusion_prompt)\n",
        "    return conclusion\n",
        "\n",
        "# problem_statement = get_problem_statement(proposal)\n",
        "# print(conclusion)"
      ],
      "metadata": {
        "id": "k0hjVQg_fAUR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = llm.invoke(f\"\"\"\n",
        "generate a press release for a startup that is working on the following proposal:\n",
        "{proposal}\n",
        "\n",
        "in the format\n",
        "Biotech and AI startup Cradle is finding success with its generative approach to protein design, landing big customers and a hefty $24 million of new investment.\n",
        "The company exited stealth a little over a year ago, just as the hype around large language models was really heating up. Many AI companies in biotech train models to natively understand molecular structure; Cradle’s insight was that the long sequences of amino acids that make up the proteins in our bodies are akin to “like an alien programming language.”\n",
        "It may not be possible for a person to learn that language, but an AI model could — and a person could work with that instead. While they still couldn’t just say “make a protein that does this,” they could ask which of 100 interesting proteins looks most likely to survive at room temperature or an acidic environment.\n",
        "The approach seems to have caught the eye of major drug development companies like Johnson & Johnson and Novozymes. Creating a useful and functional protein from scratch is generally a pretty involved process, taking perhaps years and hundreds or thousands of wet-lab experiments.\n",
        "Cradle says its tech can significantly cut down that time and the number of experiments required. Though it did not really substantiate claims of halving development time, it did provide an illustrative example from its in-house development.\n",
        "They used their software to produce alternate versions of T7 RNA polymerase, an RNA production enzyme, that would be more resistant to high temperatures. Normally, they said, a team might expect under 5% of purposefully tweaked molecules to have the desired aspect, but 70% of the variants produced by Cradle showed increased stability. That’s the equivalent of running four or five such experimental runs in one.\n",
        "In addition to T7, Cradle is working internally on “a dehalogenase that can be used to decontaminate soil, a growth factor that promotes growth through cell division commonly used in cultured meat products, a transaminase that regulates metabolic pathways and helps understand certain diseases as well as an antibody therapeutic,” said Cradle CEO and co-founder Stef van Grieken in an email to TechCrunch. “We have benchmarked our models against an in-house protein engineer using existing tools and see significant improvement in Generative AI based designs.”\n",
        "Such large improvements are possible, and small, even fractional improvements would be welcomed by the companies investing millions in these processes. But of course there is more to the drug development process than generating likely candidate molecules.\n",
        "“We have already been able to showcase the potential of our platform to accelerate the R&D phase and help our partners to bring bio-based products to market faster and more cost-effectively,” said van Grieken. “In fact, as we ourselves and several partners have now completed several rounds of experimentation on our platform, we’re seeing models generalizing very well across different types of proteins and tasks, which is incredibly exciting.”\n",
        "The tech is by no means limited to drug development and could be used in food and industrial applications as well. As with other tools of this type, part of the draw for customers is that Cradle doesn’t require a machine learning engineer to operate, but can be put directly in the hands of scientists and labs.\n",
        "I asked van Grieken his thoughts on building an EU-based biotech company (many on the team previously worked at big tech firms in Silicon Valley).\n",
        "“We have found that building in the EU has pros and cons. Fundraising for a deep-tech venture in Europe is more complicated in Europe than in the US, where there are many more modern ‘tech-bio’ investors that are interested in companies like Cradle. There is also a much larger community of like-minded founders in the Bay Area,” he said.\n",
        "“However, from a talent perspective I think Europe is underappreciated,” van Grieken continued. “For example, here in Zurich, you have all major big tech companies (Apple, Google, Facebook) represented with thousands of engineers. You have a fantastic talent pool coming out of ETH and EPFL, which are some of the best universities for computer science and molecular biology in the world. And competition for talent is definitely less intense than in the Bay Area. Finally, many of the largest pharma and biotech companies in the world are located in Europe, so we are close to our customers. I definitely think the European ecosystem is developing rapidly.”\n",
        "Cradle’s $24 million A round follows a $5.5 million seed last year. Previous investor Index Ventures led the round, with Kindred Capital (also a seed investor) participating, along with individual investors Chris Gibson, Tom Glocer and others. The company says it will use the capital to grow its team and sales, as you do.\n",
        "\"\"\")\n",
        "\n",
        "print(press_release)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "KsPKC7fegcAO",
        "outputId": "886e7919-969b-4482-d6d7-6e6a6fee4693"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Startup Revolutionizing Information Retrieval with Cross-Encoder for Retrieval-Augmented Generation\n",
            "\n",
            "CITY, DATE — Our AI startup is thrilled to announce a significant breakthrough in information retrieval with our state-of-the-art cross-encoder for retrieval-augmented generation. This cutting-edge technology integrates advanced natural language processing techniques to understand and process complex queries, delivering precise, contextually relevant information in real-time.\n",
            "\n",
            "Our cross-encoder technology has been designed with scalability in mind, enhancing search engines, recommendation systems, and data analysis tools. By providing unmatched accuracy and speed in retrieving the exact information or content users need, we empower businesses and developers to unlock the full potential of their data. Our mission is to make information access seamless and more intuitive than ever before.\n",
            "\n",
            "Since exiting stealth a little over a year ago, our solution has already gained traction among major industry players. Companies like Johnson & Johnson and Novozymes have recognized the value of our technology, which significantly cuts down the time and number of experiments required for information retrieval.\n",
            "\n",
            "Our AI model has demonstrated its ability to produce alternate versions of proteins with desired aspects, such as increased stability. In a recent example, we used our software to produce alternate versions of T7 RNA polymerase, an RNA production enzyme, that would be more resistant to high temperatures. The results showed a remarkable 70% of the variants produced by our technology exhibited increased stability, equivalent to running four or five experimental runs in one.\n",
            "\n",
            "Our technology is not limited to drug development but can also be applied in food and industrial applications. As a user-friendly tool, Cradle does not require a machine learning engineer to operate, making it accessible to scientists and labs directly.\n",
            "\n",
            "With a $24 million A round following a $5.5 million seed last year, we will use the capital to grow our team and sales. Previous investors, including Index Ventures, Kindred Capital, Chris Gibson, Tom Glocer, and others, have participated in the round.\n",
            "\n",
            "For more information about our AI startup and the cross-encoder for retrieval-augmented generation, please visit our website at [www.yourstartupwebsite.com](http://www.yourstartupwebsite.com).\n",
            "\n",
            "###\n",
            "\n",
            "About Us\n",
            "\n",
            "Our AI startup is a leader in information retrieval, leveraging advanced natural language processing techniques to deliver precise, contextually relevant information in real-time. Our mission is to empower businesses and developers to unlock the full potential of their data, making information access seamless and more intuitive than ever before.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Report"
      ],
      "metadata": {
        "id": "b0hyidUThHU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proposal = \"Smart contract platform for frictionless real estate transactions.\"\n",
        "problem_statement = get_problem_statement(proposal)\n",
        "tasks = get_research_tasks(proposal)\n",
        "citations = get_relevant_documents(proposal)\n",
        "agent_executor = get_agent_executor(llm, tools, agent_prompt)\n",
        "summaries, citations = get_research(proposal, agent_executor, tasks)\n",
        "highlights = generate_highlights(proposal, citations, summaries)\n",
        "followup_response = get_followup_questions(highlights)\n",
        "conclusion = get_conclusion(proposal, highlights)\n",
        "tasks_str = \"- \" + \"\\n- \".join(tasks)\n",
        "final_report = f\"\"\"\n",
        "## Problem Statement\n",
        "{problem_statement}\n",
        "\n",
        "## Scope of Tasks\n",
        "{tasks_str}\n",
        "\n",
        "## Research\n",
        "{highlights}\n",
        "\n",
        "## Follow up Questions\n",
        "{followup_response}\n",
        "\n",
        "## Conclusion\n",
        "{conclusion}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QiDmB9AUhJCx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_report)\n",
        "\n",
        "# It gives everything 6.5-7 rating\n",
        "# It rates everything as rollercoaster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM6gouyjhqMc",
        "outputId": "d007efa3-61f6-41a3-8373-fbb9fb06ad11"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Problem Statement\n",
            "Our startup addresses the problem of lengthy and complex real estate transactions by providing a user-friendly, secure, and efficient smart contract platform, streamlining the process and reducing the risk of errors or fraud.\n",
            "\n",
            "## Scope of Tasks\n",
            "- Legal and regulatory frameworks for smart contracts in real estate: Investigating the legal and regulatory landscape will help assess the feasibility and risks associated with implementing smart contracts in real estate transactions.\n",
            "- Current challenges and limitations of smart contract platforms in real estate: Identifying existing issues and limitations will provide insights into potential areas of improvement and innovation for the startup's proposal.\n",
            "- Adoption rates and market potential of smart contract platforms in real estate: Understanding the current market and growth potential will help evaluate the startup's opportunity for success and scalability.\n",
            "- Security and privacy concerns in smart contract-based real estate transactions: Examining potential security and privacy risks will help assess the startup's ability to address these concerns and ensure the safety and trustworthiness of their platform.\n",
            "- Technical feasibility of integrating AI and blockchain technologies in real estate transactions: Investigating the compatibility and synergies between AI and blockchain technologies will help evaluate the technical viability and innovation potential of the startup's proposal.\n",
            "\n",
            "## Research\n",
            "### Identify Key Challenges\n",
            "- Scalability of smart contract platforms in real estate\n",
            "- Security and regulatory compliance of blockchain-based transactions\n",
            "- Usability and user experience for non-technical stakeholders\n",
            "- Interoperability with existing real estate systems and platforms\n",
            "- Legal and contractual issues related to smart contracts\n",
            "\n",
            "### Explore Potential Solutions\n",
            "- Implement advanced AI techniques for improving scalability and security\n",
            "- Collaborate with regulatory bodies to ensure compliance and develop industry best practices\n",
            "- Invest in user-centered design to enhance usability and user experience\n",
            "- Develop APIs and integration tools for seamless interoperability with other platforms\n",
            "- Engage legal experts to address contractual and regulatory challenges\n",
            "\n",
            "### Assess Innovation and Trend Alignment\n",
            "- The proposal aligns with the trend of digital transformation in the real estate industry\n",
            "- AI-enabled intelligent assistants are increasingly being adopted in education, indicating potential for similar applications in real estate\n",
            "- Blockchain and smart contracts have the potential to revolutionize real estate transactions by increasing transparency, efficiency, and security\n",
            "\n",
            "### Solution Potential and Risk Evaluation\n",
            "- High potential for disruption in the real estate industry with the adoption of smart contract platforms\n",
            "- Risks associated with regulatory compliance, security, and user acceptance should be carefully managed\n",
            "- The startup should focus on building strategic partnerships, investing in R&D, and continuously iterating the product to address emerging challenges and opportunities.\n",
            "\n",
            "## Follow up Questions\n",
            "1. How do you plan to address the scalability challenges of smart contract platforms in real estate?\n",
            "2. What measures will you take to ensure the security and regulatory compliance of blockchain-based transactions?\n",
            "3. How do you intend to make the platform user-friendly for non-technical stakeholders in real estate?\n",
            "4. What strategies are in place for seamless integration with existing real estate systems and platforms?\n",
            "5. How will you tackle legal and contractual issues related to smart contracts in real estate?\n",
            "6. Can you share your plans for collaborating with regulatory bodies and legal experts?\n",
            "7. How do you plan to invest in R&D to continuously improve the product and address emerging challenges?\n",
            "8. What is your strategy for building strategic partnerships in the real estate industry?\n",
            "9. How do you plan to manage risks associated with user acceptance and regulatory compliance?\n",
            "10. Can you provide more details on your plans for investing in user-centered design and advanced AI techniques?\n",
            "\n",
            "## Conclusion\n",
            "Conclusion: The AI startup's proposal for a smart contract platform in real estate presents a high potential for disruption, but also carries significant technology risks. Key challenges include scalability, security, regulatory compliance, usability, interoperability, and legal issues. The startup should prioritize addressing these challenges through AI, collaboration with regulators, user-centered design, integration tools, and legal expertise. Despite these risks, the proposal aligns with industry trends and has the potential to revolutionize real estate transactions.\n",
            "\n",
            "Final Thoughts: It's like trying to tame a wild horse - it's a risky endeavor, but with the right tools and expertise, it could become a valuable asset.\n",
            "\n",
            "Rating: 7.5/10\n",
            "\n",
            "Emoji: 🐎💼\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fV87S-WpjG9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "216199f1a1294fff872b1c28cb54c627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c10344b10f74299ae6ebaffeedd1738",
              "IPY_MODEL_dd90efc9f84f40bd84c01fd58b71be0c",
              "IPY_MODEL_dd53297019734702a192656b88fd8950"
            ],
            "layout": "IPY_MODEL_72e069e47d524f25aaf4ee0f0cc03eb7"
          }
        },
        "5c10344b10f74299ae6ebaffeedd1738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1887361a8bcb4ce2b4a067ed965a9dd3",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c9b3939d364424a250979c40e0e4b2",
            "value": "Fetching 13 files: 100%"
          }
        },
        "dd90efc9f84f40bd84c01fd58b71be0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de7acd9af01a42bd8f5e539d959484c2",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c86150808174e1090089321c4f41a6c",
            "value": 13
          }
        },
        "dd53297019734702a192656b88fd8950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ba4e735abe4527a79f13d708603f21",
            "placeholder": "​",
            "style": "IPY_MODEL_19eb0fc431fa442498e5e025d28e54cb",
            "value": " 13/13 [00:00&lt;00:00, 1082.49it/s]"
          }
        },
        "72e069e47d524f25aaf4ee0f0cc03eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1887361a8bcb4ce2b4a067ed965a9dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c9b3939d364424a250979c40e0e4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de7acd9af01a42bd8f5e539d959484c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c86150808174e1090089321c4f41a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70ba4e735abe4527a79f13d708603f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19eb0fc431fa442498e5e025d28e54cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7cb02b343bb45b4a095d26caec0c2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b47dbe2e63e4151b1b6c40e33f53ddd",
              "IPY_MODEL_69c44660f89a4598b3fe8cdd515d3c52",
              "IPY_MODEL_c9ca7f6af1794655ba3879da496a6f77"
            ],
            "layout": "IPY_MODEL_833aa6ac259d4b7a879327399fd95c0e"
          }
        },
        "5b47dbe2e63e4151b1b6c40e33f53ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec85f1bc6644323bd99ca672c79b941",
            "placeholder": "​",
            "style": "IPY_MODEL_a14f476418224be29dca0c795e61e30f",
            "value": " 51%"
          }
        },
        "69c44660f89a4598b3fe8cdd515d3c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_766f136cf3b44efda50db7511e7a548b",
            "max": 22996,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d168d1f438448359da5a329d100507d",
            "value": 11656
          }
        },
        "c9ca7f6af1794655ba3879da496a6f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e3fe09e691c4faa9582082ddfd6ede0",
            "placeholder": "​",
            "style": "IPY_MODEL_f6fe02a1134a49409cda85223f21935f",
            "value": " 11656/22996 [1:21:10&lt;1:30:12,  2.10it/s]"
          }
        },
        "833aa6ac259d4b7a879327399fd95c0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec85f1bc6644323bd99ca672c79b941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14f476418224be29dca0c795e61e30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "766f136cf3b44efda50db7511e7a548b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d168d1f438448359da5a329d100507d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e3fe09e691c4faa9582082ddfd6ede0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6fe02a1134a49409cda85223f21935f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}